{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9dc8e3",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Machine (SVM) is a\n",
    "linear classifier, meaning that it produces a hyperplane in vector space that attempts to separate the two classes of the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbce4233",
   "metadata": {},
   "source": [
    "The difference between logistic regression and SVMs is the loss function.\n",
    "Logistic regression uses a log-likelihood function \n",
    "that penalizes all points proportional to the error \n",
    "in the probability estimate, \n",
    "even those on the correct side of the hyperplane.\n",
    "\n",
    "An SVM, on the other hand, uses hinge loss, \n",
    "which only penalizes points on the wrong side \n",
    "of the hyperplane or very close to it on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1af3f7",
   "metadata": {},
   "source": [
    "# What is Support Vector Machine?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7d201d",
   "metadata": {},
   "source": [
    "The SVM (Support Vector Machine) classifier attempts \n",
    "to find the maximum margin hyperplane separating the\n",
    "two classes,\n",
    "where margin indicates the distance between the \n",
    "separation plane and the closest data points on either side.\n",
    "\n",
    "In the case where the data is not linearly separable,\n",
    "the points in the margin are penalized proportionally\n",
    "to their distance from the margin.\n",
    "\n",
    "\n",
    "The figure below shows a concrete example:\n",
    "the two classes are represented by white and black dots\n",
    "respectively. The solid line is the separation plane\n",
    "and the dotted lines are the margins.\n",
    "\n",
    "\n",
    "diagram:\n",
    "\n",
    "\n",
    "\n",
    "The square points are the support vectors;\n",
    "that is, those which provide a non-zero contribution\n",
    "to the loss function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14f287f6",
   "metadata": {},
   "source": [
    "How SVM Work?\n",
    "To classify a new data point x, we simply determine which side of the plane x falls on. If we want to get a real-valued score we can compute the distance from x to the separating plane and then apply a sigmoid to map to [0,1]. \n",
    "\n",
    "The real power of SVMs comes from the kernel trick, which is a mathematical transformation that takes a linear decision boundary and produces a nonlinear boundary. At a high level, a kernel transforms one vector space to another space.\n",
    "Support Vector Machine: Benefits and Limitations\n",
    "SVMs have shown very good performance in practice, especially in large spaces, and the fact that they can be described in terms of support vectors leads to efficient implementations for marking new data points.\n",
    "\n",
    "However, the complexity of forming a kernel SVM grows quadratically with the number of training samples, so that for training set sizes greater than a few million, the kernels are rarely used and the decision limit is linear.\n",
    "\n",
    "Another drawback is that the scores produced by SVMs cannot be interpreted as probabilities; converting scores to probabilities requires additional computation and cross-validation, for example using Platt scaling or isotonic regression.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
